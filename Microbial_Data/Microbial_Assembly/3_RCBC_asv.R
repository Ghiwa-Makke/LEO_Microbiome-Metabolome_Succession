### Calculates RCBC for ICR

# Loading libraries

library(tidyverse); packageVersion("tidyverse") # for dataframe processing
library(vegan); packageVersion("vegan") # for ecological applications
#library(ggtree)
library(picante)
library(phyloseq)
library(adephylo)
library(doParallel)

## Working directory

#setwd("/xdisk/tfaily/vfreirezapata/Assembly_MAGs/Assembly_AVS_updated_2024/")

## Load data 

tree <- read_tree('input/phylogenetic_tree.tree')

## Working with rarefied matrix

abundance_matrix <- read_csv("input/ASV_matrix_rarefied_LEO.csv")

matrix_ready <- abundance_matrix %>% 
  dplyr::select(sampleid, abundance, ASV) %>% 
  pivot_wider(names_from = 'ASV', values_from = 'abundance') %>%
  #mutate(sampleid = str_replace(sampleid, "-", "_")) %>% 
  column_to_rownames(var = 'sampleid')


matrix_ready <- matrix_ready[,colSums(matrix_ready > 0) >= 4]


metadata <- read_csv("input/metadata_samples.csv") 


## Rerooting the tree between Archaea and Bacteria 


## Checking if tree is rooted

is.rooted(tree)

## Rooting tree

tree_root <- phytools::midpoint.root(tree)

is.rooted(tree_root)


### Sanity check

all(rownames(matrix_ready) %in% metadata$sampleid)

## TRUE


## Calculating relative abundance of ASV
## Based on what Hannah did in my paper for MAGs coverage


rel_abundance <- as.data.frame(t(matrix_ready)) %>% 
  rownames_to_column(var = 'ASV') %>% 
  pivot_longer(!ASV, names_to = 'sampleid', values_to = 'counts') %>% 
  filter(counts > 0) %>% 
  group_by(sampleid) %>% 
  mutate(relative_abun = counts / sum(counts)) %>% 
  select(ASV, sampleid, relative_abun) %>% 
  pivot_wider(names_from = sampleid, values_from = relative_abun, values_fill = 0) %>% 
  column_to_rownames(var = 'ASV')

# Matching OTUs and tree


## Community data matrix = MAGs as rownames and Samples as colnames

phylo_match <- match.phylo.data(tree_root, rel_abundance)

#"Dropping taxa from the data because they are not present in the phylogeny:"
## Total 1801 ASV dropped out - keeping 15061


## Setting parallel execution


cl <- parallel::makeCluster(94)
registerDoParallel(cl)

# Step 1: Prepare data to calculate null RCBC communities


# Setting up the data to mesh with the Stegen et al. code
#spXsite = t(input$otu_table[,2:10]) # test with smaller sample set
#dim(spXsite)

spXsite = t(rel_abundance) 
dim(spXsite)


# if relative abundance is true add 1 to all values greater than zero otherwise 
# raup crick gets cranky

spXsite[spXsite>0] = spXsite[spXsite>0]+1


# Count number of sites and total species richness across all plots (gamma diversity)
n_sites = nrow(spXsite)
gamma = ncol(spXsite)

# Build a site-by-site matrix to hold the results, with the names of the sites in the row and col names:

results = matrix(data=NA, nrow=n_sites, ncol=n_sites, dimnames=list(row.names(spXsite), row.names(spXsite)))

# Make the spXsite matrix into a new, pres/abs. matrix to get species occurences:

spXsite.inc = ceiling(spXsite/max(spXsite))

# Create an occurrence vector - used to give more weight to widely distributed species in the null model

occur = apply(spXsite.inc, MARGIN=2, FUN=sum)

# Create an abundance vector - used to give more weight to abundant species in the second step of the null model

abundance = apply(spXsite, MARGIN=2, FUN=sum)

# Step 2: Create null pairwise comparisons (beta.reps number of times) for each sample

# Function definitions for step 2
# First set up functions to create independent site_comb combinations so each can be run
# in parallel on a super computer; the calculation for each site-site combination is independent so this is fine

beta.reps <- 1000

create_spec_comb <- function(nsites = nrow(spXsite), nreps = beta.reps) {
  # This function creates a list of site combinations that
  # describe the lower-half of a sitexsite matrix, while excluding the 
  # self-comparison diagonal. In addition, it repeats each sitexsite comparison
  # nreps number of times. The list can then be used in the create_null function
  # below to parallelize (via apply) each site-by-site null comparison.
  z <- sequence(nsites)
  
  mat_comb <- cbind(
    null.two = unlist(lapply(2:nsites, function(x) x:nsites), use.names = FALSE),
    null.one = rep(z[-length(z)], times = rev(tail(z, -1))-1)) # a two-site matrix for our null comparisons
  
  #   if(speedup == TRUE) {
  #   stoc_samples <- unlist(betaNTI_stoc[lower.tri(betaNTI_stoc)]) # vector of samples where non-stochastic are NA and stochastic are betaNTI score
  #   mat_comb <- mat_comb[!is.na(stoc_samples),] # only keep samples with betaNTI between -2 and 2
  # }
  
  
  #mat_rep <- mat_comb %x% rep(1, nreps) # mathematical "kronecker product"; essentially repeats matrix in interdigitated way, repeating each row beta.reps # of times
  #  mat_comb <- cbind(mat_comb, ntimes = rep(nreps, nrow(mat_comb)))
  mat_rep <- matrix(unlist(lapply(mat_comb, rep, nreps)), ncol = 2)
  
  nrow_mat_comb <- nrow(mat_comb)
  mat_rep_iter <- cbind(mat_rep[,c(1,2)], iter=rep(1:nreps, times = nrow_mat_comb))
  
  #mat_rep_iter.list <- as.list(data.frame(t(mat_rep_iter)))
  #mat_rep_iter.list <- lapply(seq_len(nrow(mat_rep_iter)), function(i) mat_rep_iter[i,])
  # mat_rep_iter <- matrix(as.numeric(unlist(mat_rep_iter)),nrow=nrow(mat_rep_iter))
  return(mat_rep_iter)
}


# create null creates two null communities and calculates bray-curtis distance between them for a site1-site2-iteration combination
create_null <- function(site_comb, spXsite = spXsite) {
  # This function creates the null comparisons between sites
  # by first comparing the observed number of species in each of the two sites and
  # and weighing them by their occurrence frequencies, but otherwise randomizing;
  # the species that are in the community. Then the two sites are combined into a
  # matrix, and bray-curtis distance is measured for the two-site comparision. The
  # results are saved along with the sites being compared and the beta.rep iteration.
  #create_null <- function(site_comb, spXsite = spXsite) {
  # This function creates the null comparisons between sites
  # by first comparing the observed number of species in each of the two sites and
  # and weighing them by their occurrence frequencies, but otherwise randomizing;
  # the species that are in the community. Then the two sites are combined into a
  # matrix, and bray-curtis distance is measured for the two-site comparision. The
  # results are saved along with the sites being compared and the beta.rep iteration.
  #
  # ### Function Arguments: ###
  # site_comb: is a list of site combinations and iterations, created by
  #            create_spec_comb() function, it takes the format of a list of 
  #            vectors each vector has a length of three and the first item is 
  #            "null.two" (ranges from 2:number_of_sites), the second item is 
  #            "null.one" (ranges from 1:number_of_sites-1), the third is the 
  #            replicate for the null distribution (ranges from 
  #            1:number_of_beta_reps). Together null.one and null.two describe 
  #            bottom triangle of a site-by-site matrix (excluding 
  #            self-comparisons)
  # Here's an example of what site_comb could look like:
  # 
  # List of 2340
  # $ X1   : num [1:3] 2 1 1
  # $ X2   : num [1:3] 2 1 2
  # $ X3   : num [1:3] 2 1 3
  # $ X4   : num [1:3] 2 1 4
  # etc....
  # 
  # spXsite: is the species by site matrix
  null.two <- unlist(site_comb)[1]
  null.one <- unlist(site_comb)[2]
  iter <- unlist(site_comb)[3]
  #print(paste(iter, null.one, null.two))
  #writeLines("Data read in")
  
  # Generates two empty communities of size gamma (# species in regional pool)
  com1<-rep(0,gamma)
  com2<-rep(0,gamma)
  #writeLines("Empty communities generated")
  # Add observed number of species to com1, weighting by species occurrence frequencies
  # In first sample, choose a random subset of species to form the community: x = choose from any of the gamma # of species in the region, size = choose a vector of species with a size equal to the number of species in original community; Weigh the probability of choosing a particular species by its regional occurrence frequency ("occur"); Then assign this random subset of regional species a presence value of "1"
  com1[sample(1:gamma, sum(spXsite.inc[null.one,]), replace=FALSE, prob=occur)]<-1
  
  # using the pool of randomly chosen species in com1, randomly sample x species, where x is difference in # of total individuals (or abundance counts) in the real community one and the total number of observations (pres/abs) in the real community (which is equivalent to sum(com1), and sum(spXite.inc[null.one,])).
  # The chance of choosing a species is propotionate to their abundances in the real regional community,however species may be chosen with replacement, so it is possible to pick the same species twice or more
  com1.samp.sp <- sample(which(com1>0), (sum(spXsite[null.one,])-sum(com1)), 
                         replace=TRUE, prob=abundance[which(com1>0)]); # random draws of species from the com1 community; chosen propotionate to their regional abundances. The total size of the community is bounded by the size of the real community membership
  com1.samp.sp <- cbind(com1.samp.sp,1); # hist(com1.samp.sp[,1]) = randomly generated species distribution for com1
  com1.samp.sp
  # count the number of occurences for each species
  com1.sp.counts <- as.data.frame(tapply(com1.samp.sp[,2], com1.samp.sp[,1], FUN=sum)); colnames(com1.sp.counts) = 'counts'; # head(com1.sp.counts);
  com1.sp.counts$sp <- as.numeric(rownames(com1.sp.counts));
  com1[com1.sp.counts$sp] <- com1[com1.sp.counts$sp] + com1.sp.counts$counts;
  #sum(com1) - sum(spXsite[null.one,]); # This should be zero if everything worked properly
  rm('com1.samp.sp','com1.sp.counts');			
  #writeLines("Community 1 calculated")
  
  # Again for com2
  com2[sample(1:gamma, sum(spXsite.inc[null.two,]), replace=FALSE, prob=occur)]<-1
  com2.samp.sp <- sample(which(com2>0), (sum(spXsite[null.two,])-sum(com2)), replace=TRUE, prob=abundance[which(com2>0)]);
  com2.samp.sp <- cbind(com2.samp.sp,1);
  com2.sp.counts <- as.data.frame(tapply(com2.samp.sp[,2], com2.samp.sp[,1], FUN=sum)); colnames(com2.sp.counts) = 'counts'; # head(com2.sp.counts);
  com2.sp.counts$sp <- as.numeric(rownames(com2.sp.counts));
  com2[com2.sp.counts$sp] <- com2[com2.sp.counts$sp] + com2.sp.counts$counts;
  # sum(com2) - sum(spXsite[null.two,]); # This should be zero if everything worked properly
  rm('com2.samp.sp','com2.sp.counts');
  #writeLines("Community 2 calculated")
  
  # Bind the two null communities together to create 2-samplexspecies table for the null
  # communities
  null.spXsite <- rbind(com1,com2); # Null.spXsite
  #writeLines("null df created")
  
  # Calculates the null Bray-Curtis
  temp_null_bray_curtis <- as.numeric(vegdist(null.spXsite, method='bray'));
  #writeLines("ran vegan")
  
  # Final report out:
  null_results <- list("null_bc" = temp_null_bray_curtis, 
                       "site1" = null.one,
                       "site2" = null.two,
                       "null_iter" = iter)
  #print(unlist(null_results))
  return(null_results)
}


#### Running functions

# Now run those functions: 
# Create list of site-by-site comparisons (quick doesn't need to be run in parallel)


site_comb_list <- create_spec_comb(nsites = nrow(spXsite), nreps = beta.reps)

paral <- TRUE
n.cores <- 94
max_chunk_size <- 1000

# Create an iterator for the foreach loop, that speeds up parallization
if(paral == TRUE) {
  # # Define the iterator interpretor function
  # idivix <- function(n, chunkSize) {
  #   i <- 1
  #   it <- idiv(n, chunkSize=chunkSize)
  #   nextEl <- function() {
  #     m <- nextElem(it)  # may throw 'StopIterator'
  #     value <- list(i=i, m=m)
  #     i <<- i + m
  #     value
  #     }
  #   obj <- list(nextElem=nextEl)
  #   class(obj) <- c('abstractiter', 'iter')
  #   obj
  #   }
  # Define the iterator interpretor function
  # this one will return slices of a list no larger than chunkSize
  idivix <- function(n, chunkSize, comb_list) {
    i <- 1
    it <- idiv(n, chunkSize=chunkSize)
    nextEl <- function() {
      m <- nextElem(it)  # may throw 'StopIterator'
      
      value <- list(i=i, m=m, comb = comb_list[seq(i, length.out=m),])
      i <<- i + m
      value
    }
    obj <- list(nextElem=nextEl)
    class(obj) <- c('abstractiter', 'iter')
    obj
  }
  
} else {
  iter_seq <- 1:length(site_comb_list)
}

chnk_size <- min(floor(nrow(site_comb_list)/n.cores), max_chunk_size)
writeLines(paste("To speed up processing, chunk size is set to", chnk_size, "which means there will be", nrow(site_comb_list)/chnk_size, "tasks to complete."))

print(nrow(site_comb_list))
str(site_comb_list)

# Should the program skip the generation of null bray curtis, and instead read it in from a file?
skip_null <- FALSE

outputs.fp <- 'output/null_rcbc'

if(!skip_null) {
  null_bray_curtis.ls <- foreach(a=idivix(nrow(site_comb_list), 
                                          chunkSize=chnk_size,
                                          comb_list=site_comb_list),
                                 .packages = c("vegan"),
                                 .init = NULL,
                                 .combine = rbind,
                                 .multicombine = TRUE,
                                 .maxcombine = 100,
                                 .inorder = FALSE,
                                 .errorhandling = "remove",
                                 #                                 .export = ls(globalenv()),
                                 .verbose = TRUE) %dopar% {
                                   # if(paral == T) {
                                   #   paste("Running on worker", mpi.comm.rank(0)) # testing
                                   # }                             
                                   do.call('rbind', lapply(seq(1, length.out=a$m), function(i) {
                                     site_comb_temp <- as.list(a$comb[i,])
                                     create_null(site_comb = site_comb_temp, spXsite = spXsite)}))
                                   #site_comb_temp <- site_comb_list[i]
                                   #create_null(site_comb = site_comb_temp, spXsite = spXsite)
                                 }
  writeLines("Finished calculating null replicates at:")
  print(date())
  
  head(null_bray_curtis.ls) # Check
  tail(null_bray_curtis.ls) # Check
  #str(null_bray_curtis.ls) # Check
  
  ## Prepare outputs to pass along to next step
  # Unlist output
  null_bray_curtis <- matrix(unlist(null_bray_curtis.ls), ncol = 4)
  colnames(null_bray_curtis) <- c("null_bc", "site1", "site2", "null_iter")
  null_bray_curtis <- as.data.frame(null_bray_curtis)
  head(null_bray_curtis) # debugging check, should be a dataframe with 4 columns
  # str(null_bray_curtis) # debugging check
  # null_bray_curtis[ which(null_bray_curtis$site1==1 & null_bray_curtis$site2==2), "null_bc"] # debugging check; should be length of beta.reps and show the null comparisions for sites 1 and 2
  
  
  
  ## Save objects mid-process incase we have to restart.
  
  saveRDS(null_bray_curtis,  paste0(outputs.fp, "/null_bray_curtis.RDS"))
  write.csv(null_bray_curtis,
            paste0(outputs.fp, "/null_bray_curtis_lf.csv"),
            quote=FALSE)
}


# Step 3: Compare null comparisons to the observed comparisons

# If skipping null generation:
if(skip_null) {
  writeLines("Reading in null bray curtis from:")
  print(paste0(outputs.fp, "/null_bray_curtis.RDS"))
  writeLines(paste0("null_bray_curtis.RDS was created: ", 
                    file.info(paste0(outputs.fp, "/null_bray_curtis.RDS"))$ctime))
  
  null_bray_curtis <- readRDS(paste0(outputs.fp, "/null_bray_curtis.RDS"))
  
}

# Function definition


compare_null <- function(site_comb, spXsite = spXsite) {
  # This function compares the null comparisons with the observations
  #
  # ### Function Arguments: ###
  # site_comb: is a list of site combinations and iterations, created by
  #            create_spec_comb() function, it takes the format of a list of 
  #            vectors each vector has a length of three and the first item is 
  #            "null.two" (ranges from 2:number_of_sites), the second item is 
  #            "null.one" (ranges from 1:number_of_sites-1), the third is the 
  #            replicate for the null distribution (ranges from 
  #            1:number_of_beta_reps). Together null.one and null.two describe 
  #            bottom triangle of a site-by-site matrix (excluding 
  #            self-comparisons)
  # Here's an example of what site_comb could look like:
  # 
  # List of 2340
  # $ X1   : num [1:3] 2 1 1
  # $ X2   : num [1:3] 2 1 2
  # $ X3   : num [1:3] 2 1 3
  # $ X4   : num [1:3] 2 1 4
  # etc....
  # 
  # spXsite: is the species by site matrix
  null.two <- unlist(site_comb)[1]
  null.one <- unlist(site_comb)[2]
  iter <- unlist(site_comb)[3]
  #print(paste(iter, null.one, null.two))
  
  # Calculates the observed Bray-Curtis
  obs.bray = vegdist(spXsite[c(null.one,null.two),], method='bray');
  
  # Extract the null bray curtis vector for the site under calculation
  null.bc.site = null_bray_curtis[ which(null_bray_curtis$site1==null.one
                                         & null_bray_curtis$site2==null.two), "null_bc"]
  
  # How many null observations is the observed value tied with?
  num_exact_matching_in_null = sum(null.bc.site==obs.bray);
  
  # How many null values are smaller than the observed *dissimilarity*?
  num_less_than_in_null = sum(null.bc.site<obs.bray);
  
  rc = ((num_less_than_in_null + (num_exact_matching_in_null)/2)/beta.reps) # This variation of rc splits ties
  
  rc = (rc-.5)*2 # Adjusts the range of the  Raup-Crick caclulation to -1 to 1
  
  #results[null.two,null.one] = round(rc,digits=2); # Stores rc into the results matrix
  # Final report out:
  results.lf <- list("RCBC" = round(rc, digits = 2), 
                     "site1" = null.one,
                     "site2" = null.two)
  return(results.lf)
}

# Create list of site-by-site comparisons
# nreps should = 1 since we only need to do a null-obs comparision for each pair of sites one time


rm(site_comb_list)
site_comb_list <- create_spec_comb(nsites = nrow(spXsite), nreps = 1)
head(site_comb_list)
# Use foreach to compare null to obs in parallel
chnk_size <- min(floor(nrow(site_comb_list)/n.cores),	max_chunk_size) # we don't want to exceed the max_chunk_size
#chnk_size <- max(n.cores+1, chnk_size) # n.cores represents the low end of tasks we want to run
writeLines(paste("To speed up processing, chunk size is set to", chnk_size, "which means there will be", nrow(site_comb_list)/chnk_size, "tasks to complete."))

print(length(site_comb_list))
str(site_comb_list)

results.lf.ls <- foreach(a=idivix(nrow(site_comb_list), 
                                  chunkSize=chnk_size,
                                  comb_list=site_comb_list),
                         .packages = c("vegan"),
                         .init = NULL,
                         .combine = rbind,
                         .multicombine = TRUE,
                         .maxcombine = 100,
                         .inorder = FALSE,
                         .errorhandling = "remove",
                         #                         .export = ls(globalenv()),
                         .verbose = TRUE) %dopar% {
                           # if(paral == T) {
                           #   paste("Running on worker", mpi.comm.rank(0)) # testing
                           # }
                           do.call('rbind', lapply(seq(1, length.out=a$m), function(i) {
                             site_comb_temp <- as.list(a$comb[i,])
                             compare_null(site_comb = site_comb_temp, spXsite = spXsite)}))                         
                           #site_comb_temp <- site_comb_list[i]
                           #compare_null(site_comb = site_comb_temp, spXsite = spXsite)
                         }

head(results.lf.ls) # debugging check; should be a list with 3 items
str(results.lf.ls) # debugging check

# Unlist outputs from loop
results.lf <- matrix(unlist(results.lf.ls), ncol = 3)
colnames(results.lf) <- c("RCBC", "site1", "site2")
results.lf <- as.data.frame(results.lf)
# head(results.lf) # debugging check; should be dataframe with 3 columns
# str(results.lf) # debugging check


# convert results dataframe (in long format) to distance matrix (in wide format)
# for convenience. We will save and write out both of these products
self_comp <- data.frame(site1 = rep(1:nrow(spXsite)),
                        site2 = rep(1:nrow(spXsite)),
                        RCBC = NA) # self-comparisons
# # combine self comparisons to results and spread to distance matrix format
results.wf <- results.lf %>%
  select(RCBC, site1, site2) %>%
  bind_rows(self_comp) %>%
  arrange(site1, site2) %>%
  mutate(site1 = as.character(site1),
         site2 = as.character(site2)) %>%
  pivot_wider(names_from = c(site1),
              values_from = RCBC) %>%
  column_to_rownames("site2")
# rownames(results.wf) <- row.names(spXsite)
# colnames(results.wf) <- row.names(spXsite)
head(results.wf) # debugging check

colnames(results.wf) <- rownames(spXsite)
rownames(results.wf) <- rownames(spXsite)


# Save Data

results.wf <- results.wf %>% 
  rownames_to_column(var = 'sampleid')

write_csv(results.wf, "output/RCBC_ASV_leo_rel_abundance.csv")

